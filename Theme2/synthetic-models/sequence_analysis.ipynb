{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizers(token_dict_1, token_dict_2):\n",
    "    node_type_token_dict = {\n",
    "        node_type: chr(i)\n",
    "        for i, node_type in enumerate(\n",
    "            set(\n",
    "                token_dict_1.keys()\n",
    "                | token_dict_2.keys()\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "\n",
    "    token_node_type_dict = {token: node_type for node_type, token in node_type_token_dict.items()}\n",
    "    return node_type_token_dict, token_node_type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_seqs(node_type_token_dict, seqs):\n",
    "    def _tokenize(path) -> str:\n",
    "        return \"\".join(list(map(lambda x: node_type_token_dict[x], path)))\n",
    "\n",
    "    tokenized_seqs = [_tokenize(seq) for seq in seqs]\n",
    "    return tokenized_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_seqs(token_node_type_dict, seqs):\n",
    "    def _detokenize(tokenized_seq):\n",
    "        return [token_node_type_dict[token] for token in tokenized_seq]\n",
    "\n",
    "    detokenized_seqs = [_detokenize(seq) for seq in seqs]\n",
    "    return detokenized_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_submatching(seqs, seq_len):\n",
    "    submatches = set(seqs)\n",
    "    for i, j in itertools.combinations(seqs, 2):\n",
    "        matcher = SequenceMatcher(None, i, j)\n",
    "        matching_blocks = list(matcher.get_matching_blocks())\n",
    "        for matching_block in matching_blocks:\n",
    "            a, b, size = matching_block.a, matching_block.b, matching_block.size\n",
    "            if matching_block.size > 3:\n",
    "                match_1 = i[a : a + size]\n",
    "                submatches.add(match_1)\n",
    "                if i in submatches:\n",
    "                    submatches.remove(i)\n",
    "                if j in submatches:\n",
    "                    submatches.remove(j)\n",
    "    print(len(submatches))\n",
    "    if seq_len - len(submatches) > 0:\n",
    "        return recursive_submatching(set(submatches), len(submatches))\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unq_sequences(seqs):\n",
    "    total_sequences = 0\n",
    "    unique_sequences = set()\n",
    "    for i, (_, sequences) in enumerate(seqs['results'].items()):\n",
    "        total_sequences += len(sequences)\n",
    "        unique_sequences |= set(sequences)\n",
    "\n",
    "    unique_sequences_stripped = set()\n",
    "    for seq in unique_sequences:\n",
    "        new_seq = seq.strip(seqs['node_type_token_dict']['input'])\n",
    "        new_seq = new_seq.strip(seqs['node_type_token_dict']['output'])\n",
    "        if len(new_seq) > 2:\n",
    "            unique_sequences_stripped.add(new_seq)\n",
    "\n",
    "    unique_sequences_decoded = []\n",
    "    for sequence in unique_sequences_stripped:\n",
    "        decoded_sequence = [seqs['token_node_type_dict'][i] for i in sequence]\n",
    "        unique_sequences_decoded.append(decoded_sequence)\n",
    "\n",
    "    print(f\"Total Sequences: {total_sequences:,}, Total Unique Sequences: {len(unique_sequences_stripped):,}, Total Paths Compared: {seqs['total_path_pairs_analyzed']:,}\")\n",
    "    return unique_sequences_stripped, unique_sequences_decoded, total_sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./onnx_parsing_results/torch_mismatch_seq_match_results.json', 'r') as f:\n",
    "    mismatched_seq = json.load(f)\n",
    "\n",
    "with open('./onnx_parsing_results/torch_correct_mismatch_seq_match_results.json', 'r') as f:\n",
    "    mismatched_correct = json.load(f)\n",
    "\n",
    "with open('./onnx_parsing_results/torch_test_mismatch_seq_match_results.json', 'r') as f:\n",
    "    mismatched_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mismatched Ops:\", len(mismatched_seq['node_type_token_dict'].keys()))\n",
    "print(\"Mismatched-Correct Ops:\", len(mismatched_correct['node_type_token_dict'].keys()))\n",
    "print(\"Mismatched-Test Ops:\", len(mismatched_test['node_type_token_dict'].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched_seq['node_type_token_dict'].keys() - mismatched_correct['node_type_token_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched_test['node_type_token_dict'].keys() - mismatched_correct['node_type_token_dict'].keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mismatched Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    unique_mismatched_sequences,\n",
    "    unique_mismatched_sequences_decoded,\n",
    "    total_mismatched_sequences,\n",
    ") = get_unq_sequences(mismatched_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recursive_submatching(unique_mismatched_sequences, len(unique_mismatched_sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct-Mismatched Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    unique_corr_mismatched_sequences,\n",
    "    unique_corr_mismatched_sequences_decoded,\n",
    "    total_corr_mismatched_sequences,\n",
    ") = get_unq_sequences(mismatched_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recursive_submatching(unique_corr_mismatched_sequences, len(unique_corr_mismatched_sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Tokenizers and Calculate Non-Overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_token_dict, token_node_type_dict = create_tokenizers(\n",
    "    mismatched_correct[\"node_type_token_dict\"], mismatched_seq[\"node_type_token_dict\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched_sequences = set(tokenize_seqs(node_type_token_dict, unique_mismatched_sequences_decoded))\n",
    "mismatched_correct_sequences = set(tokenize_seqs(node_type_token_dict, unique_corr_mismatched_sequences_decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonoverlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = mismatched_sequences - mismatched_correct_sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize_seqs(token_node_type_dict, seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_sequences = recursive_submatching(seqs, len(seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize_seqs(token_node_type_dict, reduced_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = list(\n",
    "    filter(\n",
    "        lambda x: (len(x) > 2),\n",
    "        detokenize_seqs(token_node_type_dict, reduced_sequences),\n",
    "    )\n",
    ")\n",
    "len(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ops = set()\n",
    "for filt in filtered:\n",
    "    filtered_ops = filtered_ops.union(filt)\n",
    "filtered_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mismatched-Test Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    unique_test_mismatched_sequences,\n",
    "    unique_test_mismatched_sequences_decoded,\n",
    "    total_test_mismatched_sequences,\n",
    ") = get_unq_sequences(mismatched_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recursive_submatching(unique_test_mismatched_sequences, len(unique_test_mismatched_sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Tokenizers and Calculate Non-Overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_token_dict, token_node_type_dict = create_tokenizers(\n",
    "    mismatched_test[\"node_type_token_dict\"], mismatched_seq[\"node_type_token_dict\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched_sequences = set(tokenize_seqs(node_type_token_dict, unique_mismatched_sequences_decoded))\n",
    "mismatched_test_sequences = set(tokenize_seqs(node_type_token_dict, unique_test_mismatched_sequences_decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonoverlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = mismatched_sequences - mismatched_test_sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_sequences = recursive_submatching(seqs, len(seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize_seqs(token_node_type_dict, reduced_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(\n",
    "    filter(\n",
    "        lambda x: (len(x) > 2) and (\"input\" not in x) and (\"output\" not in x),\n",
    "        detokenize_seqs(token_node_type_dict, reduced_sequences),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./onnx_parsing_results/tf2onnx_mismatch_seq_match_results.json', 'r') as f:\n",
    "    mismatched_seq = json.load(f)\n",
    "with open('./onnx_parsing_results/tf2onnx_correct_mismatch_seq_match_results.json', 'r') as f:\n",
    "    mismatched_correct = json.load(f)\n",
    "with open('./onnx_parsing_results/tf2onnx_test_mismatch_seq_match_results.json', 'r') as f:\n",
    "    mismatched_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mismatched Ops:\", len(mismatched_seq['node_type_token_dict'].keys()))\n",
    "print(\"Mismatched-Correct Ops:\", len(mismatched_correct['node_type_token_dict'].keys()))\n",
    "print(\"Mismatched-Test Ops:\", len(mismatched_test['node_type_token_dict'].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mismatched Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    unique_mismatched_sequences,\n",
    "    unique_mismatched_sequences_decoded,\n",
    "    total_mismatched_sequences,\n",
    ") = get_unq_sequences(mismatched_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recursive_submatching(unique_mismatched_sequences, len(unique_mismatched_sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct-Mismatched Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    unique_corr_mismatched_sequences,\n",
    "    unique_corr_mismatched_sequences_decoded,\n",
    "    total_corr_mismatched_sequences,\n",
    ") = get_unq_sequences(mismatched_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recursive_submatching(unique_corr_mismatched_sequences, len(unique_corr_mismatched_sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Tokenizers and Calculate Non-Overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_token_dict, token_node_type_dict = create_tokenizers(\n",
    "    mismatched_correct[\"node_type_token_dict\"], mismatched_seq[\"node_type_token_dict\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched_sequences = set(tokenize_seqs(node_type_token_dict, unique_mismatched_sequences_decoded))\n",
    "mismatched_correct_sequences = set(tokenize_seqs(node_type_token_dict, unique_corr_mismatched_sequences_decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonoverlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = mismatched_sequences - mismatched_correct_sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_sequences = recursive_submatching(seqs, len(seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = detokenize_seqs(token_node_type_dict, reduced_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(x, './tf_reduced_seq.pt')\n",
    "torch.save([node_type_token_dict, token_node_type_dict], './tf_tokenizer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mismatched-Test Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    unique_test_mismatched_sequences,\n",
    "    unique_test_mismatched_sequences_decoded,\n",
    "    total_test_mismatched_sequences,\n",
    ") = get_unq_sequences(mismatched_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recursive_submatching(unique_test_mismatched_sequences, len(unique_test_mismatched_sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Tokenizers and Calculate Non-Overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_token_dict, token_node_type_dict = create_tokenizers(\n",
    "    mismatched_test[\"node_type_token_dict\"], mismatched_seq[\"node_type_token_dict\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched_sequences = set(tokenize_seqs(node_type_token_dict, unique_mismatched_sequences_decoded))\n",
    "mismatched_test_sequences = set(tokenize_seqs(node_type_token_dict, unique_test_mismatched_sequences_decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonoverlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mismatched_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mismatched_test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mismatched_sequences - mismatched_test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs =  mismatched_sequences - mismatched_test_sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_sequences = recursive_submatching(seqs, len(seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(\n",
    "    filter(\n",
    "        lambda x: (len(x) > 2) and (\"input\" not in x) and (\"output\" not in x),\n",
    "        detokenize_seqs(token_node_type_dict, reduced_sequences),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
